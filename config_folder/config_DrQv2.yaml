
defaults:
  - _self_
  - agent@_global_: DrqV2
  - override hydra/launcher: submitit_local
  
  
#misc
use_tb: true
seed: 1
device: cuda
GUI: false
save_snapshot: false

# pre-initialize policy using a policy learnt from demonstr
path2data: store_expert_data/2022.08.02/100034_GUI=false/expert_data.pkl

# train settings
num_seed_steps: 100
num_train_steps: 10000

# interrupt when rollout goes too bad
early_stop: false

# env
accuracy_error_weight: 2
side_pick_only: true
add_noise: false
image_width: 160
image_height: 160
n_channels: 3

# eval
eval_every_episodes: 10
num_eval_episodes: 3

# replay buffer
replay_buffer_size: ${num_train_steps}
nstep: 1
batch_size: 16

#Agent
discount: 0.99
learning_rate: 1e-4
critic_target_tau: 0.005
update_every_steps: 1

#discriminator 
GAN_loss: least-square
learning_rate_discriminator: 4e-4
feature_dim: 100
hidden_dim: 1024
reward_d_coef: 2.0

#other options
imitation_learning: false
RL: false
augmentation: false

hydra:
  run:
    dir: ./experiments/train_DrQv2/GAN_loss_${GAN_loss}_augmentation_${augmentation}/${now:%H%M%S}_${hydra.job.override_dirname}
  sweep:
    dir: ./experiments/exp_multirun_train_DrQv2/GAN_loss_${GAN_loss}_augmentation_${augmentation}/
    subdir: ${now:%H%M%S}_${hydra.job.override_dirname}
  launcher:
    timeout_min: 18000000
    cpus_per_task: 10
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 160
    nodes: 1
    submitit_folder: ./experiments/exp_multirun_train_DrQv2/GAN_loss_${GAN_loss}_augmentation_${augmentation}/${now:%Y.%m.%d}_${now:%H%M}

