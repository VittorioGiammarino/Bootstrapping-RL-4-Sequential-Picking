
defaults:
  - _self_

# misc
use_tb: true
seed: 1
device: cuda
GUI: true
save_snapshot: true

# pre-initialize policy using a policy learnt from demonstr
train_from_imitation: true
path2policy_imit: 'train_from_demonstr/2022.08.02/140243_GUI=false,total_number_of_training_steps=1000/snapshot_GUI=False,image_width=160,image_height=160.pt'

# number of rollouts used for training
total_number_of_training_steps: 100

# interrupt when rollout goes too bad
early_stop: false

#env
accuracy_error_weight: 2
side_pick_only: true
add_noise: true
image_width: 160
image_height: 160
n_channels: 3

#MLP
feature_dim: 100
hidden_dim: 1024

#Policy
decoder_nc: 1

#Value Net
TD: false

#GAE
num_episodes_per_rollout: 1
gae_gamma: 0.99
gae_lambda: 0.99

#PPO
learning_rate: 1e-4
epsilon: 0.1
c1: 1
c2: 1e-5

#optimization
num_epochs: 1
batch_size: 256
minibatch_size: 16

#exploration terms
entropy: true

# policy to evaluate
evaluate_only: false
eval_only_iterations: 10
path2policy_eval: ???

hydra:
  run:
    dir: ./train_RL_PPO/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname}

